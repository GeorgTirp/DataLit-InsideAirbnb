{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10380336,"sourceType":"datasetVersion","datasetId":6430266}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nilsklute/kaggle-airbnb-preprocessing?scriptVersionId=216864629\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2025-01-09T19:05:05.088639Z","iopub.execute_input":"2025-01-09T19:05:05.088965Z","iopub.status.idle":"2025-01-09T19:05:05.118387Z","shell.execute_reply.started":"2025-01-09T19:05:05.088935Z","shell.execute_reply":"2025-01-09T19:05:05.117425Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/berlin-amsterdam/raw_data/amsterdam/calendar.csv\n/kaggle/input/berlin-amsterdam/raw_data/amsterdam/listings.csv\n/kaggle/input/berlin-amsterdam/raw_data/amsterdam/neighbourhoods.geojson\n/kaggle/input/berlin-amsterdam/raw_data/amsterdam/reviews.csv\n/kaggle/input/berlin-amsterdam/raw_data/amsterdam/neighbourhoods.csv\n/kaggle/input/berlin-amsterdam/raw_data/amsterdam/summary_information/listings.csv\n/kaggle/input/berlin-amsterdam/raw_data/amsterdam/summary_information/reviews.csv\n/kaggle/input/berlin-amsterdam/raw_data/berlin/calendar.csv\n/kaggle/input/berlin-amsterdam/raw_data/berlin/listings.csv\n/kaggle/input/berlin-amsterdam/raw_data/berlin/neighbourhoods.geojson\n/kaggle/input/berlin-amsterdam/raw_data/berlin/reviews.csv\n/kaggle/input/berlin-amsterdam/raw_data/berlin/neighbourhoods.csv\n/kaggle/input/berlin-amsterdam/raw_data/berlin/summary_information/listings.csv\n/kaggle/input/berlin-amsterdam/raw_data/berlin/summary_information/reviews.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom copy import deepcopy\nimport torch \nimport transformers as tf\nfrom torch.utils.data import DataLoader\nimport json\nimport sklearn\nfrom sklearn.decomposition import PCA\nfrom tqdm import tqdm\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torchvision.models import ResNet50_Weights\nimport time \n\nDEBUG_MODE = True # determines if preprocessing is in DEBUG_MODE (no processing of file --> execution of main-function)","metadata":{"execution":{"iopub.status.busy":"2025-01-09T20:47:20.789082Z","iopub.execute_input":"2025-01-09T20:47:20.789377Z","iopub.status.idle":"2025-01-09T20:47:27.416295Z","shell.execute_reply.started":"2025-01-09T20:47:20.789351Z","shell.execute_reply":"2025-01-09T20:47:27.415276Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class InsideAirbnbDataset:\n    def __init__(\n            self,\n            raw_data_dir: str = \"C:/Users/nilsk/Dokumente/Machine Learning (MSc.)/1. Semester/Data Literacy/DataLit-InsideAirbnb/data/raw_data\",\n            process_all_cities: bool = True,\n            cities_to_process: list   = [\"berlin\"]):\n        \n        self.process_all_cities = process_all_cities\n        self.cities_to_process = cities_to_process\n\n        self.raw_data_dir = raw_data_dir\n\n        # read in raw data from raw data directory in repository\n        self.raw_data_dict = self._read_data_from_files()\n\n        # integrate the reviews from reviews df into the listings df for each city in the raw_data_dict\n        self._integrate_reviews_into_listings()\n\n        # aggregate all listings dfs from each city and store in one all_cities_listings df\n        self.all_cities_listings = self._aggregate_regional_listings_into_one_df()\n        \n    \n    def _read_data_from_files(self):\n        print(f\"reading in data from {self.raw_data_dir}\")\n        cities_in_raw_data_dir = os.listdir(self.raw_data_dir)\n\n        if not self.process_all_cities and not set(self.cities_to_process).issubset(cities_in_raw_data_dir):\n            raise ValueError(\"not all requested citys are in directory\")\n        \n        raw_data_dict = {}\n\n        if self.process_all_cities:\n            self.cities_to_process = cities_in_raw_data_dir\n        \n        for city in self.cities_to_process:\n            print(f\"collecting data for city: {city}\")\n            raw_data_dict[city] = {}\n            city_dir = self.raw_data_dir + '/' + city\n            file_names = [f for f in os.listdir(city_dir) if os.path.isfile(os.path.join(city_dir, f))]\n\n            for file_name in file_names:\n                if file_name.endswith('.csv') or file_name.endswith('.geojson') or file_name.endswith('.csv.gz'):\n                    file_path = os.path.join(city_dir, file_name)\n            \n                    # Read the file into a DataFrame\n                    if file_name.endswith('.geojson'):\n                        df = pd.read_json(file_path)  # Adjust based on the specific geojson handling\n                    else:\n                        file_name_core = file_name.split(sep=\".\")[0]\n\n                        if file_name_core == \"reviews\":\n                            index_col = 1\n                        else:\n                            index_col = 0\n                            \n                        df = pd.read_csv(file_path, index_col=index_col)\n\n                        # filter out all listings which do not have price available\n                        if file_name_core == \"listings\":\n                            df = df[df[\"price\"].notna()]\n\n                    raw_data_dict[city][file_name] = df\n\n        print(f\"collecting data process done\")\n\n        return raw_data_dict\n\n    def _integrate_reviews_into_listings(self):\n        print(f\"initializing reviews collection process and integration into city listings\")\n        cities = self.raw_data_dict.keys()\n\n        for city in cities:\n            print(f\"current city: {city}\")\n            city_listings = self.raw_data_dict[city][\"listings.csv\"]\n            city_reviews = self.raw_data_dict[city][\"reviews.csv\"]       \n            city_calendar = self.raw_data_dict[city][\"calendar.csv\"] \n\n            city_listings_indices = city_listings.index.to_list()\n            city_listings[\"comments\"] = [[] for _ in range(len(city_listings))]\n\n            for index in city_listings_indices:\n                city_index_reviews = city_reviews[city_reviews[\"listing_id\"] == index]\n                comments = city_index_reviews[\"comments\"].to_list()\n\n                comments_with_newline = []\n                for comment in comments:\n                    if type(comment) is float: #if it is nan, as nan are float values\n                        comment = \"\"\n                    comment_transformed = comment.replace('<br/>', '\\n').replace('\\r', '')\n                    comments_with_newline.append(comment_transformed)\n\n                city_listings.at[index, 'comments'] = comments_with_newline\n        \n        print(\"integration of reviews into cites listings done\")\n\n    def _aggregate_regional_listings_into_one_df(self):\n        print(\"initializing aggregation of regional listings into one dataframe\")\n        cities = self.raw_data_dict.keys()\n        all_cities_listings = []\n\n        for city in cities:\n            city_listings = self.raw_data_dict[city][\"listings.csv\"]\n            city_listings.insert(0, 'region', city)\n            all_cities_listings.append(city_listings)\n\n        all_cities_listings = pd.concat(all_cities_listings, ignore_index=True)\n        print(\"aggregation done\")\n        return all_cities_listings\n\n    def add_nlp_embedding(self, \n                          nlp_col_names = ['name', 'description', 'neighborhood_overview', 'host_about', 'amenities','comments'], \n                          batch_size = 32):\n        print(\"initializing NLP embedding process\")\n        print(f\"batch size: {batch_size}\") \n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        model_name = 'distilbert-base-multilingual-cased'\n        tokenizer = tf.AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)\n        model = tf.AutoModel.from_pretrained(model_name).to(device)\n        print(f\"embeddings are computed using transformer model: {model_name} from hugging face\")\n        \n        for nlp_col_name in nlp_col_names:\n            print(f\"current nlp column: {nlp_col_name}\")\n\n            nlp_col = self.all_cities_listings[nlp_col_name]\n            nlp_col_list = []\n\n            # convert nlp columns to a list \n            if nlp_col_name in ['name', 'description', 'neighborhood_overview', 'host_about', 'comments']:\n                nlp_col_list = nlp_col.fillna(value=\"\").to_list()\n            elif nlp_col_name == \"amenities\":\n                for amenities_raw_entry in nlp_col:\n                    amenities_collection = json.loads(amenities_raw_entry) # amenities_raw_entry is in json string format\n                    nlp_col_list.append(amenities_collection)\n            else:\n                raise ValueError(f\"no procedure found for converting {nlp_col_name} to list\")\n            \n\n            nlp_col_list_embedded = []\n\n            pooling_approach = ['amenities', 'comments']\n            # for each entry in nlp column, single embeddings are inferred for amenity_items / single reviews --> then mean pooling\n            if nlp_col_name in pooling_approach:\n                for i, entry in enumerate(tqdm(nlp_col_list)):\n                    if entry == []:\n                        entry = np.asarray([\" \"])\n                        \n                    dataloader = DataLoader(entry, batch_size=batch_size)\n                    entry_items_embeddings_list = []\n                    \n                    for batch in dataloader:\n                        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n                        with torch.no_grad():\n                            outputs = model(**inputs)\n                        embeddings = outputs.last_hidden_state[:, 0, :]\n                        embeddings = embeddings.squeeze(0).cpu().numpy()\n                        entry_items_embeddings_list.append(embeddings)\n                    \n                    embeddings_array = np.vstack(entry_items_embeddings_list)\n                    mean_pooled_embedding = np.mean(embeddings_array, axis=0)\n                    nlp_col_list_embedded.append(mean_pooled_embedding)\n                    \n            # embeddings are inferred directly for the entries of all other nlp columns\n            else:\n                dataloader = DataLoader(nlp_col_list, batch_size=batch_size)\n                elements = 0\n                for batch in tqdm(dataloader):\n                    elements += len(batch)\n                    inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n                    with torch.no_grad():\n                        outputs = model(**inputs)\n                    embeddings = outputs.last_hidden_state[:, 0, :]\n                    embeddings = embeddings.cpu().numpy()\n                    if len(list(embeddings)) != len(batch):\n                        print(embeddings.shape)\n                    nlp_col_list_embedded += list(embeddings)\n            \n            nlp_col_embedded_name = nlp_col_name + '_emb'\n            self.all_cities_listings[nlp_col_embedded_name] = nlp_col_list_embedded\n        \n        print(\"nlp embedding done\")\n    \n    def dimensionality_reduction(self, \n                                 col_names = [\n                                    'name_emb', \n                                    'description_emb', \n                                    'neighborhood_overview_emb', \n                                    'host_about_emb', \n                                    'amenities_emb',\n                                    'comments_emb',\n                                    'host_picture_emb',\n                                    'picture_emb'\n                                 ],\n                                keep_variance = 0.95):\n        \n        print(\"initializing dimensionality reduction\")\n            \n        for col_name in col_names:\n            print(f\"current embeddings: {col_name}\")\n            col = self.all_cities_listings[col_name]\n            col_array = np.asarray([np.asarray(entry) for entry in col])\n\n            pca = PCA(n_components = keep_variance, svd_solver='full')\n            pca.fit(col_array)\n            dim_red_col_array = pca.transform(col_array)\n            print(f\"used {pca.n_components_ } components for dim reduction to explain {keep_variance*100}% of the data\")\n            \n            dim_red_col_name = col_name + '_dim_red'\n            self.all_cities_listings[dim_red_col_name] = list(dim_red_col_array)\n        print(\"dimensionality reduction done\")\n\n    def add_image_embedding(self, \n                            image_url_col_names = ['host_picture_url','picture_url'], \n                            batch_size = 32, \n                            embedd_n_images = -1):\n        \n        print(\"initializing image embedding process\")\n        \n        for image_url_col_name in image_url_col_names:\n            print(f\"downloading images from web for column '{image_url_col_name}'\")\n            \n            image_url_col = self.all_cities_listings[image_url_col_name]\n            image_list = []\n            no_access_indices = []\n            image_size = (256,256)\n            \n            for i, image_url in enumerate(tqdm(image_url_col)):\n                if embedd_n_images >= 0 and i == embedd_n_images:\n                    break\n                response = requests.get(image_url)\n                \n                # NaN values are floats\n                if type(image_url) is float:\n                    no_access_indices.append(i)\n                    image_list.append(Image.new(\"RGB\", image_size))\n                else:\n                    response = requests.get(image_url)\n                    # code for successful request is 200\n                    if response.status_code == 200:\n                        image = Image.open(BytesIO(response.content)).resize(image_size)\n                        if image.mode != \"RGB\":\n                            image = image.convert('RGB')\n                        image_list.append(image)\n                    else:\n                        no_access_indices.append(i)\n                        image_list.append(Image.new(\"RGB\", image_size))\n                        #response.raise_for_status()\n    \n            print(f\"pictures from rows {no_access_indices} could not be accessed\")\n            print(\"transform images and construct dataloader\")\n    \n            normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n            \n            image_transform = transforms.Compose([\n                                            transforms.Resize(256),\n                                            transforms.CenterCrop(224),\n                                            transforms.ToTensor(),\n                                            normalize\n                                            ])\n            tensor_image_list = [image_transform(image) for image in image_list]\n    \n            data_loader = DataLoader(tensor_image_list, batch_size=batch_size)\n            \n            resnet = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n            modules = list(resnet.children())[:-1]  # remove the FC layer\n            resnet_feature_extractor = torch.nn.Sequential(*modules)\n            resnet_feature_extractor.eval()\n            \n            print(\"embedding image data using ResNet50\")\n            feature_embeddings_list = []\n       \n            for batch in tqdm(data_loader):\n                with torch.no_grad():\n                    feature_embeddings = resnet_feature_extractor(batch)\n                feature_embeddings = feature_embeddings.view(feature_embeddings.size(0), -1).numpy()\n    \n                feature_embeddings_list += list(feature_embeddings)\n    \n            col_name_core = image_url_col_name.split('_')[:-1]\n            image_col_embedded_name = '_'.join(col_name_core + ['emb'])\n            \n            # only important if embedd_n_images not -1 --> not all images get embedded\n            feature_embeddings_list_n = len(feature_embeddings_list)\n            all_listings_n = len(self.all_cities_listings)\n            diff = all_listings_n - feature_embeddings_list_n\n            for _ in range (diff):\n                feature_embeddings_list.append([]) \n                \n    \n            valid_feature_embeddings_list = deepcopy(feature_embeddings_list)[:feature_embeddings_list_n]\n            for index in no_access_indices[::-1]:\n                del valid_feature_embeddings_list[index]\n    \n            valid_feature_embeddings_array = np.asarray(valid_feature_embeddings_list)\n            mean_embedding = np.mean(valid_feature_embeddings_array, axis=0)\n            print(f\"mean_embedding: {mean_embedding}\")\n            \n            for no_access_index in no_access_indices:\n                feature_embeddings_list[no_access_index] = mean_embedding\n    \n            self.all_cities_listings[image_col_embedded_name] = feature_embeddings_list\n            \n        print(\"image embedding done\")\n    \n    def save_all_cities_listings_to_file(self, \n                                         file_name_core, \n                                         saving_dir =  'C:/Users/nilsk/Dokumente/Machine Learning (MSc.)/1. Semester/Data Literacy/DataLit-InsideAirbnb/data/preprocessed_data',\n                                         single_data_frames = False):\n        \n        self.saving_dir = saving_dir\n        file_path = saving_dir + '/' + file_name_core + '.csv'\n\n        if single_data_frames:\n            for region in self.all_cities_listings[\"region\"].unique():\n                regional_listing = self.all_cities_listings[self.all_cities_listings[\"region\"] == region]\n                file_path = saving_dir + '/' + file_name_core + f\"_{region}\" + '.csv'\n                regional_listing.to_csv(file_path)\n        else:\n            file_path = saving_dir + '/' + file_name_core + '.csv'\n            self.all_cities_listings.to_csv(file_path)\n        print(f\"all cities listings saved to path: {file_path}\")","metadata":{"execution":{"iopub.status.busy":"2025-01-09T21:11:59.705172Z","iopub.execute_input":"2025-01-09T21:11:59.705529Z","iopub.status.idle":"2025-01-09T21:11:59.735867Z","shell.execute_reply.started":"2025-01-09T21:11:59.705488Z","shell.execute_reply":"2025-01-09T21:11:59.734485Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"start = time.time()\ndata_set = InsideAirbnbDataset(raw_data_dir=\"/kaggle/input/berlin-amsterdam/raw_data\", process_all_cities= False)","metadata":{"execution":{"iopub.status.busy":"2025-01-09T21:19:59.746367Z","iopub.execute_input":"2025-01-09T21:19:59.746778Z","iopub.status.idle":"2025-01-09T21:20:13.648956Z","shell.execute_reply.started":"2025-01-09T21:19:59.746744Z","shell.execute_reply":"2025-01-09T21:20:13.648032Z"},"trusted":true},"outputs":[{"name":"stdout","text":"reading in data from /kaggle/input/berlin-amsterdam/raw_data\ncollecting data for city: berlin\ncollecting data process done\ninitializing reviews collection process and integration into city listings\ncurrent city: berlin\nintegration of reviews into cites listings done\ninitializing aggregation of regional listings into one dataframe\naggregation done\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"#print(data_set.all_cities_listings['accommodates'])\n#print(data_set.all_cities_listings.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T21:13:34.593826Z","iopub.execute_input":"2025-01-09T21:13:34.594202Z","iopub.status.idle":"2025-01-09T21:13:34.598044Z","shell.execute_reply.started":"2025-01-09T21:13:34.59417Z","shell.execute_reply":"2025-01-09T21:13:34.597157Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"data_set.add_nlp_embedding()","metadata":{"execution":{"iopub.status.busy":"2025-01-09T19:10:05.188546Z","iopub.execute_input":"2025-01-09T19:10:05.188894Z","iopub.status.idle":"2025-01-09T19:10:29.084209Z","shell.execute_reply.started":"2025-01-09T19:10:05.188865Z","shell.execute_reply":"2025-01-09T19:10:29.083083Z"},"trusted":true},"outputs":[{"name":"stdout","text":"initializing NLP embedding process\nbatch size: 32\nembeddings are computed using transformer model: distilbert-base-multilingual-cased from hugging face\ncurrent nlp column: name\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 439/439 [00:05<00:00, 77.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"current nlp column: description\n","output_type":"stream"},{"name":"stderr","text":" 47%|████▋     | 207/439 [00:17<00:19, 11.81it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-2073ee1a4912>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nlp_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-bd57db031b6a>\u001b[0m in \u001b[0;36madd_nlp_embedding\u001b[0;34m(self, nlp_col_names, batch_size)\u001b[0m\n\u001b[1;32m    166\u001b[0m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"data_set.add_image_embedding() #image_url_col_names = ['host_picture_url']","metadata":{"execution":{"iopub.status.busy":"2025-01-09T21:14:23.860658Z","iopub.execute_input":"2025-01-09T21:14:23.861046Z","iopub.status.idle":"2025-01-09T21:17:35.407655Z","shell.execute_reply.started":"2025-01-09T21:14:23.861016Z","shell.execute_reply":"2025-01-09T21:17:35.406178Z"},"trusted":true},"outputs":[{"name":"stdout","text":"initializing image embedding process\ndownloading images from web for column 'host_picture_url'\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 444/14877 [03:11<1:43:44,  2.32it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-859edcdebd08>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_image_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_url_col_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'host_picture_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-29-e5f78fae1ca3>\u001b[0m in \u001b[0;36madd_image_embedding\u001b[0;34m(self, image_url_col_names, batch_size, embedd_n_images)\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0membedd_n_images\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0membedd_n_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# NaN values are floats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":34},{"cell_type":"code","source":"data_set.dimensionality_reduction()","metadata":{"execution":{"iopub.status.busy":"2025-01-09T19:05:10.691176Z","iopub.status.idle":"2025-01-09T19:05:10.691442Z","shell.execute_reply":"2025-01-09T19:05:10.691338Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_set.save_all_cities_listings_to_file('integrated_reviews_and_nlp_image_embedding_regional_listing.csv', saving_dir=\"/kaggle/working\", single_data_frames =True)\ntotal_time = time.time() - start\nprint(f\"script took {total_time} to run\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T19:05:10.692112Z","iopub.status.idle":"2025-01-09T19:05:10.692422Z","shell.execute_reply":"2025-01-09T19:05:10.692318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical_columns = ['host_since', 'host_response_rate', 'host_acceptance_rate', 'host_listings_count', 'host_total_listings_count', 'latitude', 'longitude', \n                     'bathrooms', 'bedrooms', 'beds',\n                     'accommodates', 'price', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights','maximum_minimum_nights', 'minimum_maximum_nights',\n                     'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'availability_30', 'availability_60', 'availability_90',\n                     'availability_365', 'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review', 'last_review', 'review_scores_rating', \n                     'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', \n                     'reviews_per_month', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', \n                     'calculated_host_listings_count_shared_rooms']\ncategorical_columns = ['region', 'host_location', 'host_response_time', 'host_is_superhost', 'host_neighbourhood', 'host_has_profile_pic', 'host_identity_verified', \n                       'neighbourhood_group_cleansed', 'neighbourhood_group_cleansed', 'property_type', 'room_type', 'has_availability', 'instant_bookable'] #make list with unique values of each column here\nnatural_language_columns = ['name', 'description', 'neighborhood_overview', 'host_about', 'amenities', 'comments']\nimage_weblinks_columns = ['picture_url', 'host_picture_url']\nmeta_data_columns = ['listing_url', 'scrape_id', 'last_scraped', 'source',  'host_id', 'host_url', 'host_name', 'host_thumbnail_url', 'host_verifications', 'neighbourhood', 'calendar_last_scraped', 'license']\nnan_columns = ['calendar_updated']\n\n# not shure: host_name, difference between 'host_listings_count', 'host_total_listings_count', host_verifications\n#how to encode?: host_since as calendar information, host_neigbourhood , 'latitude' and 'longitude'; 'license' as has_license (boolean)?\n# even include? 'neighbourhood' if we have 'region' as part of df but 'neighbourhood_group_cleansed', 'neighbourhood_group_cleansed' are more exact; 'bathrooms_text' if bathroom is the same\n\n\n#which category?: 'bathrooms'\nall_listings = data_set.all_cities_listings\n#print(all_listings.columns)\n\ncategorical_uniques_n = {cat_col: len(all_listings[cat_col].unique()) for cat_col in categorical_columns}\ncategorical_uniques = {cat_col: all_listings[cat_col].unique() for cat_col in categorical_columns}\n\nnumerical_uniques_n = {num_col: len(all_listings[num_col].unique()) for num_col in numerical_columns}\n\n\nprint(numerical_uniques_n)\n#print(numerical_uniques_n['host_has_profile_pic'])","metadata":{"execution":{"iopub.status.busy":"2025-01-09T19:05:10.693056Z","iopub.status.idle":"2025-01-09T19:05:10.693302Z","shell.execute_reply":"2025-01-09T19:05:10.693206Z"},"trusted":true},"outputs":[],"execution_count":null}]}