{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10380336,"sourceType":"datasetVersion","datasetId":6430266}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nilsklute/kaggle-airbnb-preprocessing?scriptVersionId=216845580\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2025-01-09T15:16:44.672652Z","iopub.execute_input":"2025-01-09T15:16:44.672936Z","iopub.status.idle":"2025-01-09T15:16:44.697141Z","shell.execute_reply.started":"2025-01-09T15:16:44.672911Z","shell.execute_reply":"2025-01-09T15:16:44.696339Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/berlin-amsterdam/raw_data/amsterdam/calendar.csv\n/kaggle/input/berlin-amsterdam/raw_data/amsterdam/listings.csv\n/kaggle/input/berlin-amsterdam/raw_data/amsterdam/neighbourhoods.geojson\n/kaggle/input/berlin-amsterdam/raw_data/amsterdam/reviews.csv\n/kaggle/input/berlin-amsterdam/raw_data/amsterdam/neighbourhoods.csv\n/kaggle/input/berlin-amsterdam/raw_data/amsterdam/summary_information/listings.csv\n/kaggle/input/berlin-amsterdam/raw_data/amsterdam/summary_information/reviews.csv\n/kaggle/input/berlin-amsterdam/raw_data/berlin/calendar.csv\n/kaggle/input/berlin-amsterdam/raw_data/berlin/listings.csv\n/kaggle/input/berlin-amsterdam/raw_data/berlin/neighbourhoods.geojson\n/kaggle/input/berlin-amsterdam/raw_data/berlin/reviews.csv\n/kaggle/input/berlin-amsterdam/raw_data/berlin/neighbourhoods.csv\n/kaggle/input/berlin-amsterdam/raw_data/berlin/summary_information/listings.csv\n/kaggle/input/berlin-amsterdam/raw_data/berlin/summary_information/reviews.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom copy import deepcopy\nimport torch \nimport transformers as tf\nfrom torch.utils.data import DataLoader\nimport json\nimport sklearn\nfrom tqdm import tqdm\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torchvision.models import ResNet50_Weights\n\nDEBUG_MODE = True # determines if preprocessing is in DEBUG_MODE (no processing of file --> execution of main-function)","metadata":{"execution":{"iopub.status.busy":"2025-01-09T15:17:03.640754Z","iopub.execute_input":"2025-01-09T15:17:03.641065Z","iopub.status.idle":"2025-01-09T15:17:08.739814Z","shell.execute_reply.started":"2025-01-09T15:17:03.641038Z","shell.execute_reply":"2025-01-09T15:17:08.739057Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class InsideAirbnbDataset:\n    def __init__(\n            self,\n            raw_data_dir: str = \"C:/Users/nilsk/Dokumente/Machine Learning (MSc.)/1. Semester/Data Literacy/DataLit-InsideAirbnb/data/raw_data\",\n            process_all_cities: bool = True,\n            cities_to_process: list   = [\"berlin\"]):\n        \n        self.process_all_cities = process_all_cities\n        self.cities_to_process = cities_to_process\n\n        self.raw_data_dir = raw_data_dir\n\n        # read in raw data from raw data directory in repository\n        self.raw_data_dict = self._read_data_from_files()\n\n        # integrate the reviews from reviews df into the listings df for each city in the raw_data_dict\n        self._integrate_reviews_into_listings()\n\n        # aggregate all listings dfs from each city and store in one all_cities_listings df\n        self.all_cities_listings = self._aggregate_regional_listings_into_one_df()\n        \n    \n    def _read_data_from_files(self):\n        print(f\"reading in data from {self.raw_data_dir}\")\n        cities_in_raw_data_dir = os.listdir(self.raw_data_dir)\n\n        if not self.process_all_cities and not set(self.cities_to_process).issubset(cities_in_raw_data_dir):\n            raise ValueError(\"not all requested citys are in directory\")\n        \n        raw_data_dict = {}\n\n        if self.process_all_cities:\n            self.cities_to_process = cities_in_raw_data_dir\n        \n        for city in self.cities_to_process:\n            print(f\"collecting data for city: {city}\")\n            raw_data_dict[city] = {}\n            city_dir = self.raw_data_dir + '/' + city\n            file_names = [f for f in os.listdir(city_dir) if os.path.isfile(os.path.join(city_dir, f))]\n\n            for file_name in file_names:\n                if file_name.endswith('.csv') or file_name.endswith('.geojson') or file_name.endswith('.csv.gz'):\n                    file_path = os.path.join(city_dir, file_name)\n            \n                    # Read the file into a DataFrame\n                    if file_name.endswith('.geojson'):\n                        df = pd.read_json(file_path)  # Adjust based on the specific geojson handling\n                    else:\n                        file_name_core = file_name.split(sep=\".\")[0]\n\n                        if file_name_core == \"reviews\":\n                            index_col = 1\n                        else:\n                            index_col = 0\n                            \n                        df = pd.read_csv(file_path, index_col=index_col)\n\n                    raw_data_dict[city][file_name] = df\n\n        print(f\"collecting data process done\")\n\n        return raw_data_dict\n\n    def _integrate_reviews_into_listings(self):\n        print(f\"initializing reviews collection process and integration into city listings\")\n        cities = self.raw_data_dict.keys()\n\n        for city in cities:\n            print(f\"current city: {city}\")\n            city_listings = self.raw_data_dict[city][\"listings.csv\"]\n            city_reviews = self.raw_data_dict[city][\"reviews.csv\"]       \n            city_calendar = self.raw_data_dict[city][\"calendar.csv\"] \n\n            city_listings_indices = city_listings.index.to_list()\n            city_listings[\"comments\"] = [[] for _ in range(len(city_listings))]\n\n            for index in city_listings_indices:\n                city_index_reviews = city_reviews[city_reviews[\"listing_id\"] == index]\n                comments = city_index_reviews[\"comments\"].to_list()\n\n                comments_with_newline = []\n                for comment in comments:\n                    if type(comment) is float: #if it is nan, as nan are float values\n                        comment = \"\"\n                    comment_transformed = comment.replace('<br/>', '\\n').replace('\\r', '')\n                    comments_with_newline.append(comment_transformed)\n\n                city_listings.at[index, 'comments'] = comments_with_newline\n        \n        print(\"integration of reviews into cites listings done\")\n\n    def _aggregate_regional_listings_into_one_df(self):\n        print(\"initializing aggregation of regional listings into one dataframe\")\n        cities = self.raw_data_dict.keys()\n        all_cities_listings = []\n\n        for city in cities:\n            city_listings = self.raw_data_dict[city][\"listings.csv\"]\n            city_listings.insert(0, 'region', city)\n            all_cities_listings.append(city_listings)\n\n        all_cities_listings = pd.concat(all_cities_listings, ignore_index=True)\n        print(\"aggregation done\")\n        return all_cities_listings\n\n    def add_nlp_embedding(self, \n                          nlp_col_names = ['name', 'description', 'neighborhood_overview', 'host_about', 'amenities','comments'], \n                          batch_size = 32):\n        print(\"initializing NLP embedding process\")\n        print(f\"batch size: {batch_size}\") \n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        model_name = 'distilbert-base-multilingual-cased'\n        tokenizer = tf.AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)\n        model = tf.AutoModel.from_pretrained(model_name).to(device)\n        print(f\"embeddings are computed using transformer model: {model_name} from hugging face\")\n        \n        for nlp_col_name in nlp_col_names:\n            print(f\"current nlp column: {nlp_col_name}\")\n\n            nlp_col = self.all_cities_listings[nlp_col_name]\n            nlp_col_list = []\n\n            # convert nlp columns to a list \n            if nlp_col_name in ['name', 'description', 'neighborhood_overview', 'host_about', 'comments']:\n                nlp_col_list = nlp_col.fillna(value=\"\").to_list()\n            elif nlp_col_name == \"amenities\":\n                for amenities_raw_entry in nlp_col:\n                    amenities_collection = json.loads(amenities_raw_entry) # amenities_raw_entry is in json string format\n                    nlp_col_list.append(amenities_collection)\n            else:\n                raise ValueError(f\"no procedure found for converting {nlp_col_name} to list\")\n            \n\n            nlp_col_list_embedded = []\n\n            pooling_approach = ['amenities', 'comments']\n            # for each entry in nlp column, single embeddings are inferred for amenity_items / single reviews --> then mean pooling\n            if nlp_col_name in pooling_approach:\n                for i, entry in enumerate(tqdm(nlp_col_list)):\n                    if entry == []:\n                        entry = np.asarray([\" \"])\n                        \n                    dataloader = DataLoader(entry, batch_size=batch_size)\n                    entry_items_embeddings_list = []\n                    \n                    for batch in dataloader:\n                        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n                        with torch.no_grad():\n                            outputs = model(**inputs)\n                        embeddings = outputs.last_hidden_state[:, 0, :]\n                        embeddings = embeddings.squeeze(0).cpu().numpy()\n                        entry_items_embeddings_list.append(embeddings)\n                    \n                    embeddings_array = np.vstack(entry_items_embeddings_list)\n                    mean_pooled_embedding = np.mean(embeddings_array, axis=0)\n                    nlp_col_list_embedded.append(mean_pooled_embedding)\n                    \n            # embeddings are inferred directly for the entries of all other nlp columns\n            else:\n                dataloader = DataLoader(nlp_col_list, batch_size=batch_size)\n                for batch in tqdm(dataloader):\n                    inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n                    with torch.no_grad():\n                        outputs = model(**inputs)\n                    embeddings = outputs.last_hidden_state[:, 0, :]\n                    embeddings = embeddings.squeeze(0).cpu().numpy()\n                    nlp_col_list_embedded += list(embeddings)\n            \n            nlp_col_embedded_name = nlp_col_name + '_emb'\n            self.all_cities_listings[nlp_col_embedded_name] = nlp_col_list_embedded\n        \n        print(\"nlp embedding done\")\n    \n    def dimensionality_reduction(self, \n                                 col_names = [\n                                    'name_emb', \n                                    'description_emb', \n                                    'neighborhood_overview_emb', \n                                    'host_about_emb', \n                                    'amenities_emb',\n                                    'comments_emb'\n                                 ],\n                                keep_variance = 0.95):\n        \n        print(\"initializing dimensionality reduction\")\n            \n        for col_name in col_names:\n            print(f\"current embeddings: {col_name}\")\n            col = self.all_cities_listings[col_name]\n            col_array = np.asarray([np.asarray(entry) for entry in col])\n\n            pca = sklearn.decomposition.PCA(n_components = keep_variance, svd_solver='full')\n            pca.fit(col_array)\n            dim_red_col_array = pca.transform(col_array)\n            print(f\"used {pca.n_components_ } components for dim reduction to explain {keep_variance*100}% of the data\")\n            \n            dim_red_col_name = col_name + '_dim_red'\n            self.all_cities_listings[dim_red_col_name] = list(dim_red_col_array)\n        print(\"dimensionality reduction done\")\n\n    def add_image_embedding(self, \n                            image_url_col_names = ['host_picture_url','picture_url'], \n                            batch_size = 32, \n                            embedd_n_images = -1):\n        \n        print(\"initializing image embedding process\")\n        \n        for image_url_col_name in image_url_col_names:\n            print(f\"downloading images from web for column '{image_url_col_name}'\")\n            \n            image_url_col = self.all_cities_listings[image_url_col_name]\n            image_list = []\n            no_access_indices = []\n            image_size = (256,256)\n            \n            for i, image_url in enumerate(tqdm(image_url_col)):\n                if embedd_n_images >= 0 and i == embedd_n_images:\n                    break\n                response = requests.get(image_url)\n                \n                # code for successful request is 200\n                if response.status_code == 200:\n                    image = Image.open(BytesIO(response.content)).resize(image_size)\n                    if image.mode != \"RGB\":\n                        image = image.convert('RGB')\n                    image_list.append(image)\n                else:\n                    no_access_indices.append(i)\n                    image_list.append(Image.new(\"RGB\", image_size))\n                    #response.raise_for_status()\n    \n            print(f\"pictures from rows {no_access_indices} could not be accessed\")\n            print(\"transform images and construct dataloader\")\n    \n            normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n            \n            image_transform = transforms.Compose([\n                                            transforms.Resize(256),\n                                            transforms.CenterCrop(224),\n                                            transforms.ToTensor(),\n                                            normalize\n                                            ])\n            tensor_image_list = [image_transform(image) for image in image_list]\n    \n            data_loader = DataLoader(tensor_image_list, batch_size=batch_size)\n            \n            resnet = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n            modules = list(resnet.children())[:-1]  # remove the FC layer\n            resnet_feature_extractor = torch.nn.Sequential(*modules)\n            resnet_feature_extractor.eval()\n            \n            print(\"embedding image data using ResNet50\")\n            feature_embeddings_list = []\n       \n            for batch in tqdm(data_loader):\n                with torch.no_grad():\n                    feature_embeddings = resnet_feature_extractor(batch)\n                feature_embeddings = feature_embeddings.view(feature_embeddings.size(0), -1).numpy()\n    \n                feature_embeddings_list += list(feature_embeddings)\n    \n            col_name_core = image_url_col_name.split('_')[:-1]\n            image_col_embedded_name = '_'.join(col_name_core + ['emb'])\n            \n            # only important if embedd_n_images not -1 --> not all images get embedded\n            feature_embeddings_list_n = len(feature_embeddings_list)\n            all_listings_n = len(self.all_cities_listings)\n            diff = all_listings_n - feature_embeddings_list_n\n            for _ in range (diff):\n                feature_embeddings_list.append([]) \n                \n    \n            valid_feature_embeddings_list = deepcopy(feature_embeddings_list)[:feature_embeddings_list_n]\n            for index in no_access_indices[::-1]:\n                del valid_feature_embeddings_list[index]\n    \n            valid_feature_embeddings_array = np.asarray(valid_feature_embeddings_list)\n            mean_embedding = np.mean(valid_feature_embeddings_array, axis=0)\n            print(f\"mean_embedding: {mean_embedding}\")\n            \n            for no_access_index in no_access_indices:\n                feature_embeddings_list[no_access_index] = mean_embedding\n    \n            self.all_cities_listings[image_col_embedded_name] = feature_embeddings_list\n            \n        print(\"image embedding done\")\n    \n    def save_all_cities_listings_to_file(self, \n                                         file_name, \n                                         saving_dir =  'C:/Users/nilsk/Dokumente/Machine Learning (MSc.)/1. Semester/Data Literacy/DataLit-InsideAirbnb/data/preprocessed_data'):\n        \n        self.saving_dir = saving_dir\n        file_path = saving_dir + '/' + file_name\n        self.all_cities_listings.to_csv(file_path)\n        print(f\"all cities listings saved to path: {file_path}\")","metadata":{"execution":{"iopub.status.busy":"2025-01-09T15:18:51.173044Z","iopub.execute_input":"2025-01-09T15:18:51.173549Z","iopub.status.idle":"2025-01-09T15:18:51.199336Z","shell.execute_reply.started":"2025-01-09T15:18:51.173519Z","shell.execute_reply":"2025-01-09T15:18:51.198313Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"data_set = InsideAirbnbDataset(raw_data_dir=\"/kaggle/input/berlin-amsterdam/raw_data\")","metadata":{"execution":{"iopub.status.busy":"2025-01-09T15:18:58.241994Z","iopub.execute_input":"2025-01-09T15:18:58.242332Z","iopub.status.idle":"2025-01-09T15:19:28.964123Z","shell.execute_reply.started":"2025-01-09T15:18:58.242302Z","shell.execute_reply":"2025-01-09T15:19:28.963224Z"},"trusted":true},"outputs":[{"name":"stdout","text":"reading in data from /kaggle/input/berlin-amsterdam/raw_data\ncollecting data for city: amsterdam\ncollecting data for city: berlin\ncollecting data process done\ninitializing reviews collection process and integration into city listings\ncurrent city: amsterdam\ncurrent city: berlin\nintegration of reviews into cites listings done\ninitializing aggregation of regional listings into one dataframe\naggregation done\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#data_set.save_all_cities_listings_to_file('ignore_all_listings.csv', saving_dir=\"/kaggle/working\")\nprint(data_set.all_cities_listings['accommodates'])\nprint(data_set.all_cities_listings.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T16:09:23.988024Z","iopub.execute_input":"2025-01-09T16:09:23.988382Z","iopub.status.idle":"2025-01-09T16:09:23.995013Z","shell.execute_reply.started":"2025-01-09T16:09:23.988352Z","shell.execute_reply":"2025-01-09T16:09:23.994071Z"}},"outputs":[{"name":"stdout","text":"0        3\n1        2\n2        2\n3        3\n4        6\n        ..\n23671    2\n23672    2\n23673    2\n23674    3\n23675    1\nName: accommodates, Length: 23676, dtype: int64\nIndex(['region', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'name',\n       'description', 'neighborhood_overview', 'picture_url', 'host_id',\n       'host_url', 'host_name', 'host_since', 'host_location', 'host_about',\n       'host_response_time', 'host_response_rate', 'host_acceptance_rate',\n       'host_is_superhost', 'host_thumbnail_url', 'host_picture_url',\n       'host_neighbourhood', 'host_listings_count',\n       'host_total_listings_count', 'host_verifications',\n       'host_has_profile_pic', 'host_identity_verified', 'neighbourhood',\n       'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'latitude',\n       'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms',\n       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n       'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',\n       'maximum_minimum_nights', 'minimum_maximum_nights',\n       'maximum_maximum_nights', 'minimum_nights_avg_ntm',\n       'maximum_nights_avg_ntm', 'calendar_updated', 'has_availability',\n       'availability_30', 'availability_60', 'availability_90',\n       'availability_365', 'calendar_last_scraped', 'number_of_reviews',\n       'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review',\n       'last_review', 'review_scores_rating', 'review_scores_accuracy',\n       'review_scores_cleanliness', 'review_scores_checkin',\n       'review_scores_communication', 'review_scores_location',\n       'review_scores_value', 'license', 'instant_bookable',\n       'calculated_host_listings_count',\n       'calculated_host_listings_count_entire_homes',\n       'calculated_host_listings_count_private_rooms',\n       'calculated_host_listings_count_shared_rooms', 'reviews_per_month',\n       'comments', 'name_emb', 'description_emb', 'neighborhood_overview_emb',\n       'host_about_emb', 'amenities_emb'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"data_set.add_nlp_embedding()","metadata":{"execution":{"iopub.status.busy":"2025-01-09T15:35:02.280952Z","iopub.execute_input":"2025-01-09T15:35:02.281442Z","iopub.status.idle":"2025-01-09T15:47:34.089798Z","shell.execute_reply.started":"2025-01-09T15:35:02.281414Z","shell.execute_reply":"2025-01-09T15:47:34.088516Z"},"trusted":true},"outputs":[{"name":"stdout","text":"initializing NLP embedding process\nbatch size: 32\nembeddings are computed using transformer model: distilbert-base-multilingual-cased from hugging face\ncurrent nlp column: name\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 740/740 [00:09<00:00, 77.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"current nlp column: description\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 740/740 [01:04<00:00, 11.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"current nlp column: neighborhood_overview\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 740/740 [01:37<00:00,  7.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"current nlp column: host_about\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 740/740 [01:26<00:00,  8.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"current nlp column: amenities\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23676/23676 [04:03<00:00, 97.18it/s] \n","output_type":"stream"},{"name":"stdout","text":"current nlp column: comments\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 1795/23676 [04:08<50:32,  7.22it/s]  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-2073ee1a4912>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nlp_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-d5db4609524f>\u001b[0m in \u001b[0;36madd_nlp_embedding\u001b[0;34m(self, nlp_col_names, batch_size)\u001b[0m\n\u001b[1;32m    147\u001b[0m                         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m                         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    701\u001b[0m                 \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         return self.transformer(\n\u001b[0m\u001b[1;32m    704\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    462\u001b[0m                 )\n\u001b[1;32m    463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    465\u001b[0m                     \u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                     \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \"\"\"\n\u001b[1;32m    389\u001b[0m         \u001b[0;31m# Self-Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         sa_output = self.attention(\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, q_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, q_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":28},{"cell_type":"code","source":"data_set.dimensionality_reduction(col_names = ['name_emb'])","metadata":{"execution":{"iopub.status.busy":"2025-01-09T15:47:42.203417Z","iopub.execute_input":"2025-01-09T15:47:42.20372Z","iopub.status.idle":"2025-01-09T15:47:42.266336Z","shell.execute_reply.started":"2025-01-09T15:47:42.203695Z","shell.execute_reply":"2025-01-09T15:47:42.265208Z"},"trusted":true},"outputs":[{"name":"stdout","text":"initializing dimensionality reduction\ncurrent embeddings: name_emb\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-11938ba612cf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimensionality_reduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'name_emb'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-d5db4609524f>\u001b[0m in \u001b[0;36mdimensionality_reduction\u001b[0;34m(self, col_names, keep_variance)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mcol_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep_variance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvd_solver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mdim_red_col_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'sklearn' has no attribute 'decomposition'"],"ename":"AttributeError","evalue":"module 'sklearn' has no attribute 'decomposition'","output_type":"error"}],"execution_count":29},{"cell_type":"code","source":"data_set.add_image_embedding(embedd_n_images = 64)","metadata":{"execution":{"iopub.status.busy":"2025-01-09T14:04:32.829689Z","iopub.execute_input":"2025-01-09T14:04:32.830027Z","iopub.status.idle":"2025-01-09T14:04:56.519637Z","shell.execute_reply.started":"2025-01-09T14:04:32.829998Z","shell.execute_reply":"2025-01-09T14:04:56.518004Z"},"trusted":true},"outputs":[{"name":"stdout","text":"initializing image embedding process\ndownloading images from web for column 'host_picture_url'\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 64/23676 [00:04<26:51, 14.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"pictures from rows [] could not be accessed\ntransform images and construct dataloader\nembedding image data using ResNet50\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2/2 [00:08<00:00,  4.00s/it]\n","output_type":"stream"},{"name":"stdout","text":"mean_embedding: [0.06219788 0.02726397 0.10110724 ... 0.03680211 0.03957219 0.06989616]\ndownloading images from web for column 'picture_url'\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 38/23676 [00:10<1:49:59,  3.58it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-d9a363d40c47>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_image_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedd_n_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-d5db4609524f>\u001b[0m in \u001b[0;36madd_image_embedding\u001b[0;34m(self, image_url_col_names, batch_size, embedd_n_images)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;31m# code for successful request is 200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2291\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2293\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbox\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2295\u001b[0m             \u001b[0mbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"numerical_columns = ['host_since', 'host_response_rate', 'host_acceptance_rate', 'host_listings_count', 'host_total_listings_count', 'latitude', 'longitude', \n                     'bathrooms', 'bedrooms', 'beds',\n                     'accommodates', 'price', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights','maximum_minimum_nights', 'minimum_maximum_nights',\n                     'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'availability_30', 'availability_60', 'availability_90',\n                     'availability_365', 'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review', 'last_review', 'review_scores_rating', \n                     'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', \n                     'reviews_per_month', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', \n                     'calculated_host_listings_count_shared_rooms']\ncategorical_columns = ['region', 'host_location', 'host_response_time', 'host_is_superhost', 'host_neighbourhood', 'host_has_profile_pic', 'host_identity_verified', \n                       'neighbourhood_group_cleansed', 'neighbourhood_group_cleansed', 'property_type', 'room_type', 'has_availability', 'instant_bookable'] #make list with unique values of each column here\nnatural_language_columns = ['name', 'description', 'neighborhood_overview', 'host_about', 'amenities', 'comments']\nimage_weblinks_columns = ['picture_url', 'host_picture_url']\nmeta_data_columns = ['listing_url', 'scrape_id', 'last_scraped', 'source',  'host_id', 'host_url', 'host_name', 'host_thumbnail_url', 'host_verifications', 'neighbourhood', 'calendar_last_scraped', 'license']\nnan_columns = ['calendar_updated']\n\n# not shure: host_name, difference between 'host_listings_count', 'host_total_listings_count', host_verifications\n#how to encode?: host_since as calendar information, host_neigbourhood , 'latitude' and 'longitude'; 'license' as has_license (boolean)?\n# even include? 'neighbourhood' if we have 'region' as part of df but 'neighbourhood_group_cleansed', 'neighbourhood_group_cleansed' are more exact; 'bathrooms_text' if bathroom is the same\n\n\n#which category?: 'bathrooms'\nall_listings = data_set.all_cities_listings\n#print(all_listings.columns)\n\ncategorical_uniques_n = {cat_col: len(all_listings[cat_col].unique()) for cat_col in categorical_columns}\ncategorical_uniques = {cat_col: all_listings[cat_col].unique() for cat_col in categorical_columns}\n\nnumerical_uniques_n = {num_col: len(all_listings[num_col].unique()) for num_col in numerical_columns}\n\n\nprint(numerical_uniques_n)\n#print(numerical_uniques_n['host_has_profile_pic'])","metadata":{"execution":{"iopub.status.busy":"2025-01-09T14:01:22.488217Z","iopub.status.idle":"2025-01-09T14:01:22.488534Z","shell.execute_reply":"2025-01-09T14:01:22.488373Z"},"trusted":true},"outputs":[],"execution_count":null}]}