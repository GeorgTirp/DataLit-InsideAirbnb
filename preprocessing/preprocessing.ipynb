{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T22:50:27.805071Z",
     "iopub.status.busy": "2025-01-08T22:50:27.804795Z",
     "iopub.status.idle": "2025-01-08T22:50:27.838268Z",
     "shell.execute_reply": "2025-01-08T22:50:27.837279Z",
     "shell.execute_reply.started": "2025-01-08T22:50:27.805047Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/berlin-amsterdam/raw_data/amsterdam/calendar.csv\n",
      "/kaggle/input/berlin-amsterdam/raw_data/amsterdam/listings.csv\n",
      "/kaggle/input/berlin-amsterdam/raw_data/amsterdam/neighbourhoods.geojson\n",
      "/kaggle/input/berlin-amsterdam/raw_data/amsterdam/reviews.csv\n",
      "/kaggle/input/berlin-amsterdam/raw_data/amsterdam/neighbourhoods.csv\n",
      "/kaggle/input/berlin-amsterdam/raw_data/amsterdam/summary_information/listings.csv\n",
      "/kaggle/input/berlin-amsterdam/raw_data/amsterdam/summary_information/reviews.csv\n",
      "/kaggle/input/berlin-amsterdam/raw_data/berlin/calendar.csv\n",
      "/kaggle/input/berlin-amsterdam/raw_data/berlin/listings.csv\n",
      "/kaggle/input/berlin-amsterdam/raw_data/berlin/neighbourhoods.geojson\n",
      "/kaggle/input/berlin-amsterdam/raw_data/berlin/reviews.csv\n",
      "/kaggle/input/berlin-amsterdam/raw_data/berlin/neighbourhoods.csv\n",
      "/kaggle/input/berlin-amsterdam/raw_data/berlin/summary_information/listings.csv\n",
      "/kaggle/input/berlin-amsterdam/raw_data/berlin/summary_information/reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:04:00.602622Z",
     "iopub.status.busy": "2025-01-09T02:04:00.602302Z",
     "iopub.status.idle": "2025-01-09T02:04:00.608022Z",
     "shell.execute_reply": "2025-01-09T02:04:00.606923Z",
     "shell.execute_reply.started": "2025-01-09T02:04:00.602599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import torch \n",
    "import transformers as tf\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "DEBUG_MODE = True # determines if preprocessing is in DEBUG_MODE (no processing of file --> execution of main-function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:04:05.453380Z",
     "iopub.status.busy": "2025-01-09T02:04:05.453008Z",
     "iopub.status.idle": "2025-01-09T02:04:05.505276Z",
     "shell.execute_reply": "2025-01-09T02:04:05.504214Z",
     "shell.execute_reply.started": "2025-01-09T02:04:05.453348Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class InsideAirbnbDataset:\n",
    "    def __init__(\n",
    "            self,\n",
    "            raw_data_dir: str = \"C:/Users/nilsk/Dokumente/Machine Learning (MSc.)/1. Semester/Data Literacy/DataLit-InsideAirbnb/data/raw_data\",\n",
    "            process_all_cities: bool = True,\n",
    "            cities_to_process: list   = [\"berlin\"]):\n",
    "        \n",
    "        self.process_all_cities = process_all_cities\n",
    "        self.cities_to_process = cities_to_process\n",
    "\n",
    "        self.raw_data_dir = raw_data_dir\n",
    "\n",
    "        # read in raw data from raw data directory in repository\n",
    "        self.raw_data_dict = self._read_data_from_files()\n",
    "\n",
    "        # integrate the reviews from reviews df into the listings df for each city in the raw_data_dict\n",
    "        self._integrate_reviews_into_listings()\n",
    "\n",
    "        # aggregate all listings dfs from each city and store in one all_cities_listings df\n",
    "        self.all_cities_listings = self._aggregate_regional_listings_into_one_df()\n",
    "        \n",
    "    \n",
    "    def _read_data_from_files(self):\n",
    "        print(f\"reading in data from {self.raw_data_dir}\")\n",
    "        cities_in_raw_data_dir = os.listdir(self.raw_data_dir)\n",
    "\n",
    "        if not self.process_all_cities and not set(self.cities_to_process).issubset(cities_in_raw_data_dir):\n",
    "            raise ValueError(\"not all requested citys are in directory\")\n",
    "        \n",
    "        raw_data_dict = {}\n",
    "\n",
    "        if self.process_all_cities:\n",
    "            self.cities_to_process = cities_in_raw_data_dir\n",
    "        \n",
    "        for city in self.cities_to_process:\n",
    "            print(f\"collecting data for city: {city}\")\n",
    "            raw_data_dict[city] = {}\n",
    "            city_dir = self.raw_data_dir + '/' + city\n",
    "            file_names = [f for f in os.listdir(city_dir) if os.path.isfile(os.path.join(city_dir, f))]\n",
    "\n",
    "            for file_name in file_names:\n",
    "                if file_name.endswith('.csv') or file_name.endswith('.geojson') or file_name.endswith('.csv.gz'):\n",
    "                    file_path = os.path.join(city_dir, file_name)\n",
    "            \n",
    "                    # Read the file into a DataFrame\n",
    "                    if file_name.endswith('.geojson'):\n",
    "                        df = pd.read_json(file_path)  # Adjust based on the specific geojson handling\n",
    "                    else:\n",
    "                        file_name_core = file_name.split(sep=\".\")[0]\n",
    "\n",
    "                        if file_name_core == \"reviews\":\n",
    "                            index_col = 1\n",
    "                        else:\n",
    "                            index_col = 0\n",
    "                            \n",
    "                        df = pd.read_csv(file_path, index_col=index_col)\n",
    "\n",
    "                    raw_data_dict[city][file_name] = df\n",
    "\n",
    "        print(f\"collecting data process done\")\n",
    "\n",
    "        return raw_data_dict\n",
    "\n",
    "    def _integrate_reviews_into_listings(self):\n",
    "        print(f\"initializing reviews collection process and integration into city listings\")\n",
    "        cities = self.raw_data_dict.keys()\n",
    "\n",
    "        for city in cities:\n",
    "            print(f\"current city: {city}\")\n",
    "            city_listings = self.raw_data_dict[city][\"listings.csv\"]\n",
    "            city_reviews = self.raw_data_dict[city][\"reviews.csv\"]       \n",
    "            city_calendar = self.raw_data_dict[city][\"calendar.csv\"] \n",
    "\n",
    "            city_listings_indices = city_listings.index.to_list()\n",
    "            city_listings[\"comments\"] = [[] for _ in range(len(city_listings))]\n",
    "\n",
    "            for index in city_listings_indices:\n",
    "                city_index_reviews = city_reviews[city_reviews[\"listing_id\"] == index]\n",
    "                comments = city_index_reviews[\"comments\"].to_list()\n",
    "\n",
    "                comments_with_newline = []\n",
    "                for comment in comments:\n",
    "                    if type(comment) is float: #if it is nan, as nan are float values\n",
    "                        comment = \"\"\n",
    "                    comment_transformed = comment.replace('<br/>', '\\n').replace('\\r', '')\n",
    "                    comments_with_newline.append(comment_transformed)\n",
    "\n",
    "                city_listings.at[index, 'comments'] = comments_with_newline\n",
    "        \n",
    "        print(\"integration of reviews into cites listings done\")\n",
    "\n",
    "    def _aggregate_regional_listings_into_one_df(self):\n",
    "        print(\"initializing aggregation of regional listings into one dataframe\")\n",
    "        cities = self.raw_data_dict.keys()\n",
    "        all_cities_listings = []\n",
    "\n",
    "        for city in cities:\n",
    "            city_listings = self.raw_data_dict[city][\"listings.csv\"]\n",
    "            city_listings.insert(0, 'region', city)\n",
    "            all_cities_listings.append(city_listings)\n",
    "\n",
    "        all_cities_listings = pd.concat(all_cities_listings, ignore_index=True)\n",
    "        print(\"aggregation done\")\n",
    "        return all_cities_listings\n",
    "\n",
    "    def add_nlp_embedding(self, \n",
    "                          nlp_col_names = ['name', 'description', 'neighborhood_overview', 'host_about', 'amenities','comments'], \n",
    "                          batch_size = 32):\n",
    "        print(\"initializing NLP embedding process\")\n",
    "        print(f\"batch size: {batch_size}\") \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model_name = 'distilbert-base-multilingual-cased'\n",
    "        tokenizer = tf.AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)\n",
    "        model = tf.AutoModel.from_pretrained(model_name).to(device)\n",
    "        print(f\"embeddings are computed using transformer model: {model_name} from hugging face\")\n",
    "        \n",
    "        for nlp_col_name in nlp_col_names:\n",
    "            print(f\"current nlp column: {nlp_col_name}\")\n",
    "\n",
    "            nlp_col = self.all_cities_listings[nlp_col_name]\n",
    "            nlp_col_list = []\n",
    "\n",
    "            # convert nlp columns to a list \n",
    "            if nlp_col_name in ['name', 'description', 'neighborhood_overview', 'host_about', 'comments']:\n",
    "                nlp_col_list = nlp_col.fillna(value=\"\").to_list()\n",
    "            elif nlp_col_name == \"amenities\":\n",
    "                for amenities_raw_entry in nlp_col:\n",
    "                    amenities_collection = json.loads(amenities_raw_entry) # amenities_raw_entry is in json string format\n",
    "                    nlp_col_list.append(amenities_collection)\n",
    "            else:\n",
    "                raise ValueError(f\"no procedure found for converting {nlp_col_name} to list\")\n",
    "            \n",
    "\n",
    "            nlp_col_list_embedded = []\n",
    "\n",
    "            pooling_approach = ['amenities', 'comments']\n",
    "            # for each entry in nlp column, single embeddings are inferred for amenity_items / single reviews --> then mean pooling\n",
    "            if nlp_col_name in pooling_approach:\n",
    "                for i, entry in enumerate(tqdm(nlp_col_list)):\n",
    "                    if entry == []:\n",
    "                        entry = np.asarray([\" \"])\n",
    "                        \n",
    "                    dataloader = DataLoader(entry, batch_size=batch_size)\n",
    "                    entry_items_embeddings_list = []\n",
    "                    \n",
    "                    for batch in dataloader:\n",
    "                        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(**inputs)\n",
    "                        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "                        embeddings = embeddings.squeeze(0).cpu().numpy()\n",
    "                        entry_items_embeddings_list.append(embeddings)\n",
    "                    \n",
    "                    embeddings_array = np.vstack(entry_items_embeddings_list)\n",
    "                    mean_pooled_embedding = np.mean(embeddings_array, axis=0)\n",
    "                    nlp_col_list_embedded.append(mean_pooled_embedding)\n",
    "                    \n",
    "            # embeddings are inferred directly for the entries of all other nlp columns\n",
    "            else:\n",
    "                dataloader = DataLoader(nlp_col_list, batch_size=batch_size)\n",
    "                for batch in tqdm(dataloader):\n",
    "                    inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**inputs)\n",
    "                    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "                    embeddings = embeddings.squeeze(0).cpu().numpy()\n",
    "                    nlp_col_list_embedded += list(embeddings)\n",
    "            \n",
    "            nlp_col_embedded_name = nlp_col_name + '_emb'\n",
    "            self.all_cities_listings[nlp_col_embedded_name] = nlp_col_list_embedded\n",
    "        \n",
    "        print(\"nlp embedding done\")\n",
    "    \n",
    "    def dimensionality_reduction(self, \n",
    "                                 col_names = [\n",
    "                                    'name_emb', \n",
    "                                    'description_emb', \n",
    "                                    'neighborhood_overview_emb', \n",
    "                                    'host_about_emb', \n",
    "                                    'amenities_emb',\n",
    "                                    'comments_emb'\n",
    "                                 ],\n",
    "                                keep_variance = 0.95):\n",
    "        \n",
    "        print(\"initializing dimensionality reduction\")\n",
    "            \n",
    "        for col_name in col_names:\n",
    "            print(f\"current embeddings: {col_name}\")\n",
    "            col = self.all_cities_listings[col_name]\n",
    "            col_array = np.asarray([np.asarray(entry) for entry in col])\n",
    "\n",
    "            pca = sklearn.decomposition.PCA(n_components = keep_variance, svd_solver='full')\n",
    "            pca.fit(col_array)\n",
    "            dim_red_col_array = pca.transform(col_array)\n",
    "            print(f\"used {pca.n_components_ } components for dim reduction to explain {keep_variance*100}% of the data\")\n",
    "            \n",
    "            dim_red_col_name = col_name + '_dim_red'\n",
    "            self.all_cities_listings[dim_red_col_name] = list(dim_red_col_array)\n",
    "        print(\"dimensionality reduction done\")\n",
    "\n",
    "    def add_image_embedding(self, \n",
    "                            image_url_col_names = ['host_picture_url','picture_url'], \n",
    "                            batch_size = 32, \n",
    "                            embedd_n_images = -1):\n",
    "        \n",
    "        print(\"initializing image embedding process\")\n",
    "        \n",
    "        for image_url_col_name in image_url_col_names:\n",
    "            print(f\"downloading images from web for column '{image_url_col_name}'\")\n",
    "            \n",
    "            image_url_col = self.all_cities_listings[image_url_col_name]\n",
    "            image_list = []\n",
    "            no_access_indices = []\n",
    "            image_size = (256,256)\n",
    "            \n",
    "            for i, image_url in enumerate(tqdm(image_url_col)):\n",
    "                if embedd_n_images >= 0 and i == embedd_n_images:\n",
    "                    break\n",
    "                response = requests.get(image_url)\n",
    "                \n",
    "                # code for successful request is 200\n",
    "                if response.status_code == 200:\n",
    "                    image = Image.open(BytesIO(response.content)).resize(image_size)\n",
    "                    if image.mode != \"RGB\":\n",
    "                        image = image.convert('RGB')\n",
    "                    image_list.append(image)\n",
    "                else:\n",
    "                    no_access_indices.append(i)\n",
    "                    image_list.append(Image.new(\"RGB\", image_size))\n",
    "                    #response.raise_for_status()\n",
    "    \n",
    "            print(f\"pictures from rows {no_access_indices} could not be accessed\")\n",
    "            print(\"transform images and construct dataloader\")\n",
    "    \n",
    "            normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "            \n",
    "            image_transform = transforms.Compose([\n",
    "                                            transforms.Resize(256),\n",
    "                                            transforms.CenterCrop(224),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            normalize\n",
    "                                            ])\n",
    "            tensor_image_list = [image_transform(image) for image in image_list]\n",
    "    \n",
    "            data_loader = DataLoader(tensor_image_list, batch_size=batch_size)\n",
    "            \n",
    "            resnet = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "            modules = list(resnet.children())[:-1]  # remove the FC layer\n",
    "            resnet_feature_extractor = torch.nn.Sequential(*modules)\n",
    "            resnet_feature_extractor.eval()\n",
    "            \n",
    "            print(\"embedding image data using ResNet50\")\n",
    "            feature_embeddings_list = []\n",
    "       \n",
    "            for batch in tqdm(data_loader):\n",
    "                with torch.no_grad():\n",
    "                    feature_embeddings = resnet_feature_extractor(batch)\n",
    "                feature_embeddings = feature_embeddings.view(feature_embeddings.size(0), -1).numpy()\n",
    "    \n",
    "                feature_embeddings_list += list(feature_embeddings)\n",
    "    \n",
    "            col_name_core = image_url_col_name.split('_')[:-1]\n",
    "            image_col_embedded_name = '_'.join(col_name_core + ['emb'])\n",
    "            \n",
    "            # only important if embedd_n_images not -1 --> not all images get embedded\n",
    "            feature_embeddings_list_n = len(feature_embeddings_list)\n",
    "            all_listings_n = len(self.all_cities_listings)\n",
    "            diff = all_listings_n - feature_embeddings_list_n\n",
    "            for _ in range (diff):\n",
    "                feature_embeddings_list.append([]) \n",
    "                \n",
    "    \n",
    "            valid_feature_embeddings_list = deepcopy(feature_embeddings_list)[:feature_embeddings_list_n]\n",
    "            for index in no_access_indices[::-1]:\n",
    "                del valid_feature_embeddings_list[index]\n",
    "    \n",
    "            valid_feature_embeddings_array = np.asarray(valid_feature_embeddings_list)\n",
    "            mean_embedding = np.mean(valid_feature_embeddings_array, axis=0)\n",
    "            print(f\"mean_embedding: {mean_embedding}\")\n",
    "            \n",
    "            for no_access_index in no_access_indices:\n",
    "                feature_embeddings_list[no_access_index] = mean_embedding\n",
    "    \n",
    "            self.all_cities_listings[image_col_embedded_name] = feature_embeddings_list\n",
    "            \n",
    "        print(\"image embedding done\")\n",
    "    \n",
    "    def save_all_cities_listings_to_file(self, \n",
    "                                         file_name, \n",
    "                                         saving_dir =  'C:/Users/nilsk/Dokumente/Machine Learning (MSc.)/1. Semester/Data Literacy/DataLit-InsideAirbnb/data/preprocessed_data'):\n",
    "        \n",
    "        self.saving_dir = saving_dir\n",
    "        file_path = saving_dir + '/' + file_name\n",
    "        self.all_cities_listings.to_csv(file_path)\n",
    "        print(f\"all cities listings saved to path: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:04:09.454466Z",
     "iopub.status.busy": "2025-01-09T02:04:09.454151Z",
     "iopub.status.idle": "2025-01-09T02:04:40.271553Z",
     "shell.execute_reply": "2025-01-09T02:04:40.270759Z",
     "shell.execute_reply.started": "2025-01-09T02:04:09.454442Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in data from /kaggle/input/berlin-amsterdam/raw_data\n",
      "collecting data for city: amsterdam\n",
      "collecting data for city: berlin\n",
      "collecting data process done\n",
      "initializing reviews collection process and integration into city listings\n",
      "current city: amsterdam\n",
      "current city: berlin\n",
      "integration of reviews into cites listings done\n",
      "initializing aggregation of regional listings into one dataframe\n",
      "aggregation done\n"
     ]
    }
   ],
   "source": [
    "data_set = InsideAirbnbDataset(raw_data_dir=\"/kaggle/input/berlin-amsterdam/raw_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.save_all_cities_listings_to_file('ignore_all_listings.csv', saving_dir=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T22:51:12.796722Z",
     "iopub.status.busy": "2025-01-08T22:51:12.796382Z",
     "iopub.status.idle": "2025-01-08T22:51:12.800695Z",
     "shell.execute_reply": "2025-01-08T22:51:12.799591Z",
     "shell.execute_reply.started": "2025-01-08T22:51:12.796683Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#data_set.add_nlp_embedding(nlp_col_names = ['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T22:51:12.801828Z",
     "iopub.status.busy": "2025-01-08T22:51:12.801544Z",
     "iopub.status.idle": "2025-01-08T22:51:12.822915Z",
     "shell.execute_reply": "2025-01-08T22:51:12.821915Z",
     "shell.execute_reply.started": "2025-01-08T22:51:12.801804Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#data_set.dimensionality_reduction(col_names = ['name_emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:04:44.033659Z",
     "iopub.status.busy": "2025-01-09T02:04:44.033303Z",
     "iopub.status.idle": "2025-01-09T02:05:18.926113Z",
     "shell.execute_reply": "2025-01-09T02:05:18.925004Z",
     "shell.execute_reply.started": "2025-01-09T02:04:44.033615Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing image embedding process\n",
      "downloading images from web for column 'host_picture_url'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 64/23676 [00:04<27:39, 14.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pictures from rows [] could not be accessed\n",
      "transform images and construct dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 162MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding image data using ResNet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_embedding: [0.06219788 0.02726397 0.10110724 ... 0.03680211 0.03957219 0.06989616]\n",
      "downloading images from web for column 'picture_url'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 64/23676 [00:18<1:56:41,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pictures from rows [14] could not be accessed\n",
      "transform images and construct dataloader\n",
      "embedding image data using ResNet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_embedding: [0.0736902  0.01360783 0.11241758 ... 0.02339452 0.00263761 0.01632124]\n"
     ]
    }
   ],
   "source": [
    "data_set.add_image_embedding(embedd_n_images = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T22:51:12.825546Z",
     "iopub.status.busy": "2025-01-08T22:51:12.825238Z",
     "iopub.status.idle": "2025-01-08T22:51:12.890712Z",
     "shell.execute_reply": "2025-01-08T22:51:12.889675Z",
     "shell.execute_reply.started": "2025-01-08T22:51:12.825518Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'host_since': 4510, 'host_response_rate': 75, 'host_acceptance_rate': 102, 'host_listings_count': 80, 'host_total_listings_count': 90, 'latitude': 17223, 'longitude': 19193, 'bathrooms': 20, 'bedrooms': 17, 'beds': 27, 'accommodates': 16, 'price': 734, 'minimum_nights': 114, 'maximum_nights': 220, 'minimum_minimum_nights': 119, 'maximum_minimum_nights': 122, 'minimum_maximum_nights': 209, 'maximum_maximum_nights': 209, 'minimum_nights_avg_ntm': 403, 'maximum_nights_avg_ntm': 718, 'availability_30': 31, 'availability_60': 61, 'availability_90': 91, 'availability_365': 366, 'number_of_reviews': 622, 'number_of_reviews_ltm': 175, 'number_of_reviews_l30d': 40, 'first_review': 3780, 'last_review': 2307, 'review_scores_rating': 141, 'review_scores_accuracy': 135, 'review_scores_cleanliness': 173, 'review_scores_checkin': 128, 'review_scores_communication': 136, 'review_scores_location': 138, 'review_scores_value': 159, 'reviews_per_month': 824, 'calculated_host_listings_count': 37, 'calculated_host_listings_count_entire_homes': 36, 'calculated_host_listings_count_private_rooms': 18, 'calculated_host_listings_count_shared_rooms': 12}\n"
     ]
    }
   ],
   "source": [
    "numerical_columns = ['host_since', 'host_response_rate', 'host_acceptance_rate', 'host_listings_count', 'host_total_listings_count', 'latitude', 'longitude', \n",
    "                     'bathrooms', 'bedrooms', 'beds',\n",
    "                     'accommodates', 'price', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights','maximum_minimum_nights', 'minimum_maximum_nights',\n",
    "                     'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'availability_30', 'availability_60', 'availability_90',\n",
    "                     'availability_365', 'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review', 'last_review', 'review_scores_rating', \n",
    "                     'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', \n",
    "                     'reviews_per_month', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', \n",
    "                     'calculated_host_listings_count_shared_rooms']\n",
    "categorical_columns = ['region', 'host_location', 'host_response_time', 'host_is_superhost', 'host_neighbourhood', 'host_has_profile_pic', 'host_identity_verified', \n",
    "                       'neighbourhood_group_cleansed', 'neighbourhood_group_cleansed', 'property_type', 'room_type', 'has_availability', 'instant_bookable'] #make list with unique values of each column here\n",
    "natural_language_columns = ['name', 'description', 'neighborhood_overview', 'host_about', 'amenities', 'comments']\n",
    "image_weblinks_columns = ['picture_url', 'host_picture_url']\n",
    "meta_data_columns = ['listing_url', 'scrape_id', 'last_scraped', 'source',  'host_id', 'host_url', 'host_name', 'host_thumbnail_url', 'host_verifications', 'neighbourhood', 'calendar_last_scraped', 'license']\n",
    "nan_columns = ['calendar_updated']\n",
    "\n",
    "# not shure: host_name, difference between 'host_listings_count', 'host_total_listings_count', host_verifications\n",
    "#how to encode?: host_since as calendar information, host_neigbourhood , 'latitude' and 'longitude'; 'license' as has_license (boolean)?\n",
    "# even include? 'neighbourhood' if we have 'region' as part of df but 'neighbourhood_group_cleansed', 'neighbourhood_group_cleansed' are more exact; 'bathrooms_text' if bathroom is the same\n",
    "\n",
    "\n",
    "#which category?: 'bathrooms'\n",
    "all_listings = data_set.all_cities_listings\n",
    "#print(all_listings.columns)\n",
    "\n",
    "categorical_uniques_n = {cat_col: len(all_listings[cat_col].unique()) for cat_col in categorical_columns}\n",
    "categorical_uniques = {cat_col: all_listings[cat_col].unique() for cat_col in categorical_columns}\n",
    "\n",
    "numerical_uniques_n = {num_col: len(all_listings[num_col].unique()) for num_col in numerical_columns}\n",
    "\n",
    "\n",
    "print(numerical_uniques_n)\n",
    "#print(numerical_uniques_n['host_has_profile_pic'])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6430266,
     "sourceId": 10380336,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
