{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T19:57:39.403989Z",
     "iopub.status.busy": "2025-01-08T19:57:39.403678Z",
     "iopub.status.idle": "2025-01-08T19:57:39.446549Z",
     "shell.execute_reply": "2025-01-08T19:57:39.445733Z",
     "shell.execute_reply.started": "2025-01-08T19:57:39.403958Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T20:02:04.846616Z",
     "iopub.status.busy": "2025-01-08T20:02:04.846323Z",
     "iopub.status.idle": "2025-01-08T20:02:04.851158Z",
     "shell.execute_reply": "2025-01-08T20:02:04.850300Z",
     "shell.execute_reply.started": "2025-01-08T20:02:04.846595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import torch \n",
    "import transformers as tf\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEBUG_MODE = True # determines if preprocessing is in DEBUG_MODE (no processing of file --> execution of main-function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T20:03:42.490272Z",
     "iopub.status.busy": "2025-01-08T20:03:42.489903Z",
     "iopub.status.idle": "2025-01-08T20:03:42.510547Z",
     "shell.execute_reply": "2025-01-08T20:03:42.509671Z",
     "shell.execute_reply.started": "2025-01-08T20:03:42.490242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class InsideAirbnbDataset:\n",
    "    def __init__(\n",
    "            self,\n",
    "            path_to_repo: str = \"C:/Users/nilsk/Dokumente/Machine Learning (MSc.)/1. Semester/Data Literacy/DataLit-InsideAirbnb\",\n",
    "            process_all_cities: bool = True,\n",
    "            cities_to_process: list   = [\"berlin\"]):\n",
    "        \n",
    "        self.path_to_repo = path_to_repo\n",
    "        self.process_all_cities = process_all_cities\n",
    "        self.cities_to_process = cities_to_process\n",
    "\n",
    "        self.raw_data_dir = path_to_repo + '/raw_data'\n",
    "        self.saving_dir = path_to_repo + '/data/preprocessed_data'\n",
    "\n",
    "        # read in raw data from raw data directory in repository\n",
    "        self.raw_data_dict = self._read_data_from_files()\n",
    "\n",
    "        # integrate the reviews from reviews df into the listings df for each city in the raw_data_dict\n",
    "        self._integrate_reviews_into_listings()\n",
    "\n",
    "        # aggregate all listings dfs from each city and store in one all_cities_listings df\n",
    "        self.all_cities_listings = self._aggregate_regional_listings_into_one_df()\n",
    "        \n",
    "    \n",
    "    def _read_data_from_files(self):\n",
    "        print(f\"reading in data from {self.raw_data_dir}\")\n",
    "        cities_in_raw_data_dir = os.listdir(self.raw_data_dir)\n",
    "\n",
    "        if not self.process_all_cities and not set(self.cities_to_process).issubset(cities_in_raw_data_dir):\n",
    "            raise ValueError(\"not all requested citys are in directory\")\n",
    "        \n",
    "        raw_data_dict = {}\n",
    "\n",
    "        if self.process_all_cities:\n",
    "            self.cities_to_process = cities_in_raw_data_dir\n",
    "        \n",
    "        for city in self.cities_to_process:\n",
    "            print(f\"collecting data for city: {city}\")\n",
    "            raw_data_dict[city] = {}\n",
    "            city_dir = self.raw_data_dir + '/' + city\n",
    "            file_names = [f for f in os.listdir(city_dir) if os.path.isfile(os.path.join(city_dir, f))]\n",
    "\n",
    "            for file_name in file_names:\n",
    "                if file_name.endswith('.csv') or file_name.endswith('.geojson') or file_name.endswith('.csv.gz'):\n",
    "                    file_path = os.path.join(city_dir, file_name)\n",
    "            \n",
    "                    # Read the file into a DataFrame\n",
    "                    if file_name.endswith('.geojson'):\n",
    "                        df = pd.read_json(file_path)  # Adjust based on the specific geojson handling\n",
    "                    else:\n",
    "                        file_name_core = file_name.split(sep=\".\")[0]\n",
    "\n",
    "                        if file_name_core == \"reviews\":\n",
    "                            index_col = 1\n",
    "                        else:\n",
    "                            index_col = 0\n",
    "                            \n",
    "                        df = pd.read_csv(file_path, index_col=index_col)\n",
    "\n",
    "                    raw_data_dict[city][file_name] = df\n",
    "\n",
    "        print(f\"collecting data process done\")\n",
    "\n",
    "        return raw_data_dict\n",
    "\n",
    "    def _integrate_reviews_into_listings(self):\n",
    "        print(f\"initializing reviews collection process and integration into city listings\")\n",
    "        cities = self.raw_data_dict.keys()\n",
    "\n",
    "        for city in cities:\n",
    "            print(f\"current city: {city}\")\n",
    "            city_listings = self.raw_data_dict[city][\"listings.csv\"]\n",
    "            city_reviews = self.raw_data_dict[city][\"reviews.csv\"]       \n",
    "            city_calendar = self.raw_data_dict[city][\"calendar.csv\"] \n",
    "\n",
    "            city_listings_indices = city_listings.index.to_list()\n",
    "            city_listings[\"comments\"] = [[] for _ in range(len(city_listings))]\n",
    "\n",
    "            for index in city_listings_indices:\n",
    "                city_index_reviews = city_reviews[city_reviews[\"listing_id\"] == index]\n",
    "                comments = city_index_reviews[\"comments\"].to_list()\n",
    "\n",
    "                comments_with_newline = []\n",
    "                for comment in comments:\n",
    "                    if type(comment) is float: #if it is nan, as nan are float values\n",
    "                        comment = \"\"\n",
    "                    comment_transformed = comment.replace('<br/>', '\\n').replace('\\r', '')\n",
    "                    comments_with_newline.append(comment_transformed)\n",
    "\n",
    "                city_listings.at[index, 'comments'] = comments_with_newline\n",
    "        \n",
    "        print(\"integration of reviews into cites listings done\")\n",
    "\n",
    "    def _aggregate_regional_listings_into_one_df(self):\n",
    "        print(\"initializing aggregation of regional listings into one dataframe\")\n",
    "        cities = self.raw_data_dict.keys()\n",
    "        all_cities_listings = []\n",
    "\n",
    "        for city in cities:\n",
    "            city_listings = self.raw_data_dict[city][\"listings.csv\"]\n",
    "            city_listings.insert(0, 'region', city)\n",
    "            all_cities_listings.append(city_listings)\n",
    "\n",
    "        all_cities_listings = pd.concat(all_cities_listings, ignore_index=True)\n",
    "        print(\"aggregation done\")\n",
    "        return all_cities_listings\n",
    "\n",
    "\n",
    "    def add_nlp_embedding(self, \n",
    "                          nlp_col_names = ['name', 'description', 'neighborhood_overview', 'host_about', 'amenities','comments'], \n",
    "                          batch_size = 32):\n",
    "        print(\"initializing NLP embedding process\")\n",
    "        print(f\"batch size: {batch_size}\") \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model_name = 'distilbert-base-multilingual-cased'\n",
    "        tokenizer = tf.AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)\n",
    "        model = tf.AutoModel.from_pretrained(model_name).to(device)\n",
    "        print(f\"embeddings are computed using transformer model: {model_name} from hugging face\")\n",
    "        \n",
    "        for nlp_col_name in nlp_col_names:\n",
    "            print(f\"current nlp column: {nlp_col_name}\")\n",
    "\n",
    "            nlp_col = self.all_cities_listings[nlp_col_name]\n",
    "            nlp_col_list = []\n",
    "\n",
    "            # convert nlp columns to a list \n",
    "            if nlp_col_name in ['name', 'description', 'neighborhood_overview', 'host_about', 'comments']:\n",
    "                nlp_col_list = nlp_col.fillna(value=\"\").to_list()\n",
    "            elif nlp_col_name == \"amenities\":\n",
    "                for amenities_raw_entry in nlp_col:\n",
    "                    amenities_collection = json.loads(amenities_raw_entry) # amenities_raw_entry is in json string format\n",
    "                    nlp_col_list.append(amenities_collection)\n",
    "            else:\n",
    "                raise ValueError(f\"no procedure found for converting {nlp_col_name} to list\")\n",
    "            \n",
    "\n",
    "            nlp_col_list_embedded = []\n",
    "\n",
    "            pooling_approach = ['amenities', 'comments']\n",
    "            # for each entry in nlp column, single embeddings are inferred for amenity_items / single reviews --> then mean pooling\n",
    "            if nlp_col_name in pooling_approach:\n",
    "                for i, entry in enumerate(tqdm(nlp_col_list)):\n",
    "                    if entry == []:\n",
    "                        entry = np.asarray([\" \"])\n",
    "                        \n",
    "                    dataloader = DataLoader(entry, batch_size=batch_size)\n",
    "                    entry_items_embeddings_list = []\n",
    "                    \n",
    "                    for batch in dataloader:\n",
    "                        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(**inputs)\n",
    "                        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "                        embeddings = embeddings.squeeze(0).cpu().numpy()\n",
    "                        entry_items_embeddings_list.append(embeddings)\n",
    "                    \n",
    "                    embeddings_array = np.vstack(entry_items_embeddings_list)\n",
    "                    mean_pooled_embedding = np.mean(embeddings_array, axis=0)\n",
    "                    nlp_col_list_embedded.append(mean_pooled_embedding)\n",
    "                    \n",
    "            # embeddings are inferred directly for the entries of all other nlp columns\n",
    "            else:\n",
    "                dataloader = DataLoader(nlp_col_list, batch_size=batch_size)\n",
    "                for batch in tqdm(dataloader):\n",
    "                    inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**inputs)\n",
    "                    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "                    embeddings = embeddings.squeeze(0).cpu().numpy()\n",
    "                    nlp_col_list_embedded += list(embeddings)\n",
    "            \n",
    "            nlp_col_embedded_name = nlp_col_name + '_emb'\n",
    "            self.all_cities_listings[nlp_col_embedded_name] = nlp_col_list_embedded\n",
    "        \n",
    "        print(\"nlp embedding done\")\n",
    "\n",
    "    def dimensionality_reduction(self, \n",
    "                                 col_names = [\n",
    "                                    'name_emb', \n",
    "                                    'description_emb', \n",
    "                                    'neighborhood_overview_emb', \n",
    "                                    'host_about_emb', \n",
    "                                    'amenities_emb',\n",
    "                                    'comments_emb'\n",
    "                                 ],\n",
    "                                keep_variance = 0.95):\n",
    "        \n",
    "        print(\"initializing dimensionality reduction\")\n",
    "            \n",
    "        for col_name in col_names:\n",
    "            print(f\"current embeddings: {col_name}\")\n",
    "            col = self.all_cities_listings[col_name]\n",
    "            col_array = np.asarray([np.asarray(entry) for entry in col])\n",
    "\n",
    "            pca = sklearn.decomposition.PCA(n_components = keep_variance, svd_solver='full')\n",
    "            pca.fit(col_array)\n",
    "            dim_red_col_array = pca.transform(col_array)\n",
    "            print(f\"used {pca.n_components_ } components for dim reduction to explain {keep_variance*100}% of the data\")\n",
    "            \n",
    "            dim_red_col_name = col_name + '_dim_red'\n",
    "            self.all_cities_listings[dim_red_col_name] = list(dim_red_col_array)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T20:03:46.199988Z",
     "iopub.status.busy": "2025-01-08T20:03:46.199701Z",
     "iopub.status.idle": "2025-01-08T20:04:16.072932Z",
     "shell.execute_reply": "2025-01-08T20:04:16.072124Z",
     "shell.execute_reply.started": "2025-01-08T20:03:46.199967Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in data from /kaggle/input/berlin-amsterdam/raw_data\n",
      "collecting data for city: amsterdam\n",
      "collecting data for city: berlin\n",
      "collecting data process done\n",
      "initializing reviews collection process and integration into city listings\n",
      "current city: amsterdam\n",
      "current city: berlin\n",
      "integration of reviews into cites listings done\n",
      "initializing aggregation of regional listings into one dataframe\n",
      "aggregation done\n"
     ]
    }
   ],
   "source": [
    "data_set = InsideAirbnbDataset(path_to_repo=\"/kaggle/input/berlin-amsterdam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T20:04:16.076423Z",
     "iopub.status.busy": "2025-01-08T20:04:16.076100Z",
     "iopub.status.idle": "2025-01-08T20:04:26.539562Z",
     "shell.execute_reply": "2025-01-08T20:04:26.538546Z",
     "shell.execute_reply.started": "2025-01-08T20:04:16.076398Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing NLP embedding process\n",
      "batch size: 32\n",
      "embeddings are computed using transformer model: distilbert-base-multilingual-cased from hugging face\n",
      "current nlp column: name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 740/740 [00:09<00:00, 76.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp embedding done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_set.add_nlp_embedding(nlp_col_names = ['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T20:04:29.854599Z",
     "iopub.status.busy": "2025-01-08T20:04:29.853614Z",
     "iopub.status.idle": "2025-01-08T20:04:31.446541Z",
     "shell.execute_reply": "2025-01-08T20:04:31.445247Z",
     "shell.execute_reply.started": "2025-01-08T20:04:29.854553Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing dimensionality reduction\n",
      "current embeddings: name_emb\n",
      "used 158 components for dim reduction to explain 95.0% of the data\n"
     ]
    }
   ],
   "source": [
    "data_set.dimensionality_reduction(col_names = ['name_emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T20:04:46.204640Z",
     "iopub.status.busy": "2025-01-08T20:04:46.204306Z",
     "iopub.status.idle": "2025-01-08T20:04:46.254786Z",
     "shell.execute_reply": "2025-01-08T20:04:46.254038Z",
     "shell.execute_reply.started": "2025-01-08T20:04:46.204613Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['region', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'name',\n",
      "       'description', 'neighborhood_overview', 'picture_url', 'host_id',\n",
      "       'host_url', 'host_name', 'host_since', 'host_location', 'host_about',\n",
      "       'host_response_time', 'host_response_rate', 'host_acceptance_rate',\n",
      "       'host_is_superhost', 'host_thumbnail_url', 'host_picture_url',\n",
      "       'host_neighbourhood', 'host_listings_count',\n",
      "       'host_total_listings_count', 'host_verifications',\n",
      "       'host_has_profile_pic', 'host_identity_verified', 'neighbourhood',\n",
      "       'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'latitude',\n",
      "       'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms',\n",
      "       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n",
      "       'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',\n",
      "       'maximum_minimum_nights', 'minimum_maximum_nights',\n",
      "       'maximum_maximum_nights', 'minimum_nights_avg_ntm',\n",
      "       'maximum_nights_avg_ntm', 'calendar_updated', 'has_availability',\n",
      "       'availability_30', 'availability_60', 'availability_90',\n",
      "       'availability_365', 'calendar_last_scraped', 'number_of_reviews',\n",
      "       'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review',\n",
      "       'last_review', 'review_scores_rating', 'review_scores_accuracy',\n",
      "       'review_scores_cleanliness', 'review_scores_checkin',\n",
      "       'review_scores_communication', 'review_scores_location',\n",
      "       'review_scores_value', 'license', 'instant_bookable',\n",
      "       'calculated_host_listings_count',\n",
      "       'calculated_host_listings_count_entire_homes',\n",
      "       'calculated_host_listings_count_private_rooms',\n",
      "       'calculated_host_listings_count_shared_rooms', 'reviews_per_month',\n",
      "       'comments', 'name_emb', 'name_emb_dim_red'],\n",
      "      dtype='object')\n",
      "{'host_since': 4510, 'host_response_rate': 75, 'host_acceptance_rate': 102, 'host_listings_count': 80, 'host_total_listings_count': 90, 'latitude': 17223, 'longitude': 19193, 'bathrooms': 20, 'bedrooms': 17, 'beds': 27, 'accommodates': 16, 'price': 734, 'minimum_nights': 114, 'maximum_nights': 220, 'minimum_minimum_nights': 119, 'maximum_minimum_nights': 122, 'minimum_maximum_nights': 209, 'maximum_maximum_nights': 209, 'minimum_nights_avg_ntm': 403, 'maximum_nights_avg_ntm': 718, 'availability_30': 31, 'availability_60': 61, 'availability_90': 91, 'availability_365': 366, 'number_of_reviews': 622, 'number_of_reviews_ltm': 175, 'number_of_reviews_l30d': 40, 'first_review': 3780, 'last_review': 2307, 'review_scores_rating': 141, 'review_scores_accuracy': 135, 'review_scores_cleanliness': 173, 'review_scores_checkin': 128, 'review_scores_communication': 136, 'review_scores_location': 138, 'review_scores_value': 159, 'reviews_per_month': 824, 'calculated_host_listings_count': 37, 'calculated_host_listings_count_entire_homes': 36, 'calculated_host_listings_count_private_rooms': 18, 'calculated_host_listings_count_shared_rooms': 12}\n"
     ]
    }
   ],
   "source": [
    "numerical_columns = ['host_since', 'host_response_rate', 'host_acceptance_rate', 'host_listings_count', 'host_total_listings_count', 'latitude', 'longitude', \n",
    "                     'bathrooms', 'bedrooms', 'beds',\n",
    "                     'accommodates', 'price', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights','maximum_minimum_nights', 'minimum_maximum_nights',\n",
    "                     'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'availability_30', 'availability_60', 'availability_90',\n",
    "                     'availability_365', 'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review', 'last_review', 'review_scores_rating', \n",
    "                     'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', \n",
    "                     'reviews_per_month', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', \n",
    "                     'calculated_host_listings_count_shared_rooms']\n",
    "\n",
    "categorical_columns = ['region', 'host_location', 'host_response_time', 'host_is_superhost', 'host_neighbourhood', 'host_has_profile_pic', 'host_identity_verified', \n",
    "                       'neighbourhood_group_cleansed', 'neighbourhood_group_cleansed', 'property_type', 'room_type', 'has_availability', 'instant_bookable'] #make list with unique values of each column here\n",
    "\n",
    "natural_language_columns = ['name', 'description', 'neighborhood_overview', 'host_about', 'amenities', 'comments']\n",
    "\n",
    "image_weblinks_columns = ['picture_url', 'host_picture_url']\n",
    "\n",
    "meta_data_columns = ['listing_url', 'scrape_id', 'last_scraped', 'source',  'host_id', 'host_url', 'host_name', 'host_thumbnail_url', 'host_verifications', 'neighbourhood', 'calendar_last_scraped', 'license']\n",
    "\n",
    "nan_columns = ['calendar_updated']\n",
    "\n",
    "# not shure: host_name, difference between 'host_listings_count', 'host_total_listings_count', host_verifications\n",
    "#how to encode?: host_since as calendar information, host_neigbourhood , 'latitude' and 'longitude'; 'license' as has_license (boolean)?\n",
    "# even include? 'neighbourhood' if we have 'region' as part of df but 'neighbourhood_group_cleansed', 'neighbourhood_group_cleansed' are more exact; 'bathrooms_text' if bathroom is the same\n",
    "\n",
    "\n",
    "#which category?: 'bathrooms'\n",
    "all_listings = data_set.all_cities_listings\n",
    "print(all_listings.columns)\n",
    "\n",
    "categorical_uniques_n = {cat_col: len(all_listings[cat_col].unique()) for cat_col in categorical_columns}\n",
    "categorical_uniques = {cat_col: all_listings[cat_col].unique() for cat_col in categorical_columns}\n",
    "\n",
    "numerical_uniques_n = {num_col: len(all_listings[num_col].unique()) for num_col in numerical_columns}\n",
    "\n",
    "\n",
    "print(numerical_uniques_n)\n",
    "#print(numerical_uniques_n['host_has_profile_pic'])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6430266,
     "sourceId": 10380336,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
